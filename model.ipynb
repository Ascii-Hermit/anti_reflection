{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed. All images are stored in 'aug_blended' and 'aug_transmission_layer'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a more comprehensive augmentation pipeline\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.9),  # Horizontal flip with 90% probability\n",
    "    A.VerticalFlip(p=0.9),    # Vertical flip with 50% probability\n",
    "    A.RandomRotate90(p=0.9),  # Random 90-degree rotations with 50% probability\n",
    "    A.Rotate(limit=45, p=0.9),  # Random rotation between -45 and 45 degrees\n",
    "    A.RandomBrightnessContrast(p=0.5),  # Random brightness and contrast adjustment\n",
    "    A.Blur(blur_limit=7, p=0.3),  # Random blur with a blur limit\n",
    "    A.Resize(512, 512),  # Resize to ensure uniform size\n",
    "])\n",
    "\n",
    "\n",
    "# List of directories containing paired input/target subdirectories\n",
    "dataset_dirs = [r\"D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\Dataset\\DSLR\\unaligned_test50\", \n",
    "                r\"D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\Dataset\\DSLR\\unaligned_train250\",\n",
    "                r\"D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\Dataset\\Smartphone\\unaligned150\"               \n",
    "                ]  # Add paths to your directories here\n",
    "\n",
    "# Global output directories\n",
    "output_input_dir = r\"D:/MANIPAL/Research/Reflection_Removal/Code/new_aug/aug_blended\"\n",
    "output_target_dir = r\"D:/MANIPAL/Research/Reflection_Removal/Code/new_aug/aug_transmission_layer\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_input_dir, exist_ok=True)\n",
    "os.makedirs(output_target_dir, exist_ok=True)\n",
    "\n",
    "# Function to augment and save images\n",
    "def augment_and_save(input_path, target_path, output_input_dir, output_target_dir, new_name):\n",
    "    input_image = cv2.imread(input_path)\n",
    "    target_image = cv2.imread(target_path)\n",
    "\n",
    "    # Apply augmentations\n",
    "    augmented = augmentations(image=input_image, mask=target_image)\n",
    "    augmented_input = augmented['image']\n",
    "    augmented_target = augmented['mask']\n",
    "\n",
    "    # Save augmented images\n",
    "    cv2.imwrite(os.path.join(output_input_dir, new_name), augmented_input)\n",
    "    cv2.imwrite(os.path.join(output_target_dir, new_name), augmented_target)\n",
    "\n",
    "# Process each dataset directory\n",
    "image_counter = 0  # Counter to ensure unique filenames across all directories\n",
    "\n",
    "for dataset_dir in dataset_dirs:\n",
    "    input_dir = os.path.join(dataset_dir, \"blended\")\n",
    "    target_dir = os.path.join(dataset_dir, \"transmission_layer\")\n",
    "\n",
    "    # Process each pair of input/target images\n",
    "    input_files = sorted(os.listdir(input_dir))\n",
    "    target_files = sorted(os.listdir(target_dir))\n",
    "\n",
    "    for input_file, target_file in zip(input_files, target_files):\n",
    "        # Ensure input and target have the same name\n",
    "        input_path = os.path.join(input_dir, input_file)\n",
    "        target_path = os.path.join(target_dir, target_file)\n",
    "\n",
    "        # Perform 3 augmentations per image\n",
    "        for i in range(5):\n",
    "            new_name = f\"aug_image_{image_counter:04d}.jpg\"  # Unique name with a zero-padded counter\n",
    "            augment_and_save(input_path, target_path, output_input_dir, output_target_dir, new_name)\n",
    "            image_counter += 1\n",
    "\n",
    "print(\"Augmentation completed. All images are stored in 'aug_blended' and 'aug_transmission_layer'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\chris\\.vscode\\extensions\\ms-python.python-2024.20.0-win32-x64\\python_files\\python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'nn' is not defined\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.skip_connection = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip_connection(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet-style Encoder and Decoder\n",
    "class ReflectionRemovalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReflectionRemovalNet, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(3, 64, stride=1),\n",
    "            ResidualBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128, 256, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResidualBlock(256, 256, stride=1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            ResidualBlock(256, 128, stride=1),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128, 64, stride=1),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        bottleneck = self.bottleneck(enc)\n",
    "        dec = self.decoder(bottleneck)\n",
    "        return dec\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, path=r\"anti_reflection\\models\\mobilenet_v2-b0353104.pth\"):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.features = mobilenet.features\n",
    "        self.layers = nn.Sequential(*list(self.features.children())[:15]).eval()\n",
    "    \n",
    "        for param in self.layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_features = self.layers(input)\n",
    "        target_features = self.layers(target)\n",
    "        return nn.functional.l1_loss(input_features, target_features)\n",
    "\n",
    "\n",
    "# gives pixel level accuracy\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "        if loss_type == 'l1':\n",
    "            self.loss_fn = nn.L1Loss()\n",
    "        elif loss_type == 'l2':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        return self.loss_fn(predicted, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\chris\\.vscode\\extensions\\ms-python.python-2024.20.0-win32-x64\\python_files\\python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'torch' is not defined\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ReflectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reflected_dir, clear_dir, transform=None):\n",
    "        self.reflected_files = sorted(os.listdir(reflected_dir))\n",
    "        self.clear_files = sorted(os.listdir(clear_dir))\n",
    "        self.reflected_dir = reflected_dir\n",
    "        self.clear_dir = clear_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reflected_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        reflected_image = Image.open(os.path.join(self.reflected_dir, self.reflected_files[idx])).convert('RGB')\n",
    "        clear_image = Image.open(os.path.join(self.clear_dir, self.clear_files[idx])).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            reflected_image = self.transform(reflected_image)\n",
    "            clear_image = self.transform(clear_image)\n",
    "\n",
    "        return {'input': reflected_image, 'target': clear_image}\n",
    "\n",
    "# Data Preprocessing\n",
    "def get_dataloader(reflected_dir, clear_dir, batch_size=16):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = ReflectionDataset(reflected_dir=reflected_dir, clear_dir=clear_dir, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, dataloader, device, num_epochs=25, learning_rate=1e-4, output_dir=\"trained_models/\"\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # defining all the functions\n",
    "    perceptual_loss_fn = PerceptualLoss().to(device)\n",
    "    mse_loss_fn = nn.MSELoss()  # MSE Loss\n",
    "    reconstruction_loss_fn = nn.L1Loss()  \n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch > 300:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch['input'].to(device), batch['target'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            mse_loss = mse_loss_fn(outputs, targets)  \n",
    "            perceptual_loss = perceptual_loss_fn(outputs, targets) \n",
    "            reconstruction_loss = reconstruction_loss_fn(outputs, targets)  \n",
    "            \n",
    "            loss = mse_loss + 0.5 * perceptual_loss + 0.5 * reconstruction_loss #sum all hte losses\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:  # checkpointing\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"new_model_final.pth\"))\n",
    "    print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2718\n",
      "Epoch [2/100], Loss: 0.2682\n",
      "Epoch [3/100], Loss: 0.2568\n",
      "Epoch [4/100], Loss: 0.2403\n",
      "Epoch [5/100], Loss: 0.2270\n",
      "Epoch [6/100], Loss: 0.2138\n",
      "Epoch [7/100], Loss: 0.2030\n",
      "Epoch [8/100], Loss: 0.1939\n",
      "Epoch [9/100], Loss: 0.1858\n",
      "Epoch [10/100], Loss: 0.1784\n",
      "Epoch [11/100], Loss: 0.1708\n",
      "Epoch [12/100], Loss: 0.1632\n",
      "Epoch [13/100], Loss: 0.1557\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m ReflectionRemovalNet()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, device, num_epochs, learning_rate, output_dir)\u001b[0m\n\u001b[0;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     33\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m mse_loss_fn(outputs, targets)  \u001b[38;5;66;03m# MSE Loss\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m perceptual_loss \u001b[38;5;241m=\u001b[39m \u001b[43mperceptual_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perceptual Loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss_fn(outputs, targets)  \u001b[38;5;66;03m# Reconstruction Loss\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Combine losses\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 115\u001b[0m, in \u001b[0;36mPerceptualLoss.forward\u001b[1;34m(self, predicted, target)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, predicted, target):\n\u001b[0;32m    114\u001b[0m     predicted_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(predicted)\n\u001b[1;32m--> 115\u001b[0m     target_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39ml1_loss(predicted_features, target_features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reflected_dir = r\"anti_reflection\\dataset\\aug_blended\" # train images that have reflections\n",
    "    clear_dir = r\"anti_reflection\\dataset\\aug_transmission_layer\" # train images that have no reflections\n",
    "    output_dir = r\"anti_reflection\\models\" # dir to store the model\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataloader = get_dataloader(reflected_dir, clear_dir, batch_size)\n",
    "    model = ReflectionRemovalNet()\n",
    "\n",
    "    train_model(model, dataloader, device, num_epochs, learning_rate, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_23972\\2773471955.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.png and saved to D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\predicted_imgs\\0.png\n",
      "Processed aug_image_0009.jpg and saved to D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\predicted_imgs\\aug_image_0009.jpg\n",
      "Processed aug_image_0017.jpg and saved to D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\predicted_imgs\\aug_image_0017.jpg\n",
      "Processed cupboard.jpg and saved to D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\predicted_imgs\\cupboard.jpg\n",
      "Processed dada.jpg and saved to D:\\MANIPAL\\Research\\Reflection_Removal\\Code\\predicted_imgs\\dada.jpg\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, device):\n",
    "    model = ReflectionRemovalNet()  \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def process_image(image_path, transform, device):\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0) # to correct the dimension\n",
    "    return image.to(device)\n",
    "\n",
    "def save_output_image(output_tensor, save_path):\n",
    "    output_image = output_tensor.squeeze(0).cpu().detach()\n",
    "    output_image = transforms.ToPILImage()(output_image)\n",
    "    output_image.save(save_path)\n",
    "\n",
    "def test_model(model_path, input_dir, output_dir, device):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    model = load_model(model_path, device) \n",
    "\n",
    "    input_images = sorted(os.listdir(input_dir))\n",
    "    for img_name in input_images:\n",
    "        input_path = os.path.join(input_dir, img_name)\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "\n",
    "        # check if image files\n",
    "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            print(f\"Skipping non-image file: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        input_tensor = process_image(input_path, transform, device)\n",
    "\n",
    "        # generating output\n",
    "        with torch.no_grad():\n",
    "            output_tensor = model(input_tensor)\n",
    "\n",
    "        save_output_image(output_tensor, output_path)\n",
    "        print(f\"Processed {img_name} and saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = r\"anti_reflection\\models\\new_model_final.pth\"  # path to the trained model\n",
    "    input_dir = r\"anti_reflection\\dataset\\aug_blended\"  # Folder containing test reflected images\n",
    "    output_dir = r\"anti_reflection\\dataset\\predicted\"  # Folder to save output images\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    test_model(model_path, input_dir, output_dir, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(output_image, ground_truth_image):\n",
    "    mse = np.mean((ground_truth_image - output_image) ** 2)\n",
    "    if mse == 0:  \n",
    "        return float('inf')\n",
    "    max_pixel = 1.0  \n",
    "    psnr_value = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr_value\n",
    "\n",
    "def calculate_ssim(output_image, ground_truth_image):\n",
    "    \n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    output_gray = cv2.cvtColor((output_image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0\n",
    "    ground_truth_gray = cv2.cvtColor((ground_truth_image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0\n",
    "\n",
    "    mu_x = cv2.GaussianBlur(output_gray, (11, 11), 1.5)\n",
    "    mu_y = cv2.GaussianBlur(ground_truth_gray, (11, 11), 1.5)\n",
    "    sigma_x = cv2.GaussianBlur(output_gray ** 2, (11, 11), 1.5) - mu_x ** 2\n",
    "    sigma_y = cv2.GaussianBlur(ground_truth_gray ** 2, (11, 11), 1.5) - mu_y ** 2\n",
    "    sigma_xy = cv2.GaussianBlur(output_gray * ground_truth_gray, (11, 11), 1.5) - mu_x * mu_y\n",
    "\n",
    "    numerator = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)\n",
    "    denominator = (mu_x ** 2 + mu_y ** 2 + C1) * (sigma_x + sigma_y + C2)\n",
    "    ssim_map = numerator / (denominator + 1e-6)\n",
    "    return ssim_map.mean()\n",
    "\n",
    "def calculate_precision_recall_accuracy(output_image, ground_truth_image):\n",
    "    \n",
    "    output_gray = cv2.cvtColor((output_image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    ground_truth_gray = cv2.cvtColor((ground_truth_image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    _, output_binary = cv2.threshold(output_gray, 128, 1, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_binary = cv2.threshold(ground_truth_gray, 128, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "    # calculate TP, FP, FN, TN\n",
    "    tp = np.sum(output_binary * ground_truth_binary)  \n",
    "    fp = np.sum(output_binary * (1 - ground_truth_binary)) \n",
    "    fn = np.sum((1 - output_binary) * ground_truth_binary)  \n",
    "    tn = np.sum((1 - output_binary) * (1 - ground_truth_binary))  \n",
    "\n",
    "    precision = tp / (tp + fp + 1e-6)  # avoid division by zero\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn + 1e-6)\n",
    "\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"Accuracy\": accuracy}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_17980\\2390499443.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(generator_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics for Directory:\n",
      "PSNR: 16.0225\n",
      "SSIM: 0.4029\n",
      "Precision: 0.7755\n",
      "Recall: 0.7496\n",
      "Accuracy: 0.8101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def test_gan_model_on_directory(generator_path, test_dir, ground_truth_dir):\n",
    "   \n",
    "    generator = ReflectionRemovalNet().to(device)\n",
    "    generator.load_state_dict(torch.load(generator_path, map_location=device))\n",
    "    generator.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    accuracy_values = []\n",
    "\n",
    "    test_images = sorted(os.listdir(test_dir))\n",
    "    ground_truth_images = sorted(os.listdir(ground_truth_dir))\n",
    "\n",
    "    for test_img_name, gt_img_name in zip(test_images, ground_truth_images):\n",
    "        test_img_path = os.path.join(test_dir, test_img_name)\n",
    "        gt_img_path = os.path.join(ground_truth_dir, gt_img_name)\n",
    "\n",
    "        input_image = Image.open(test_img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensor = generator(input_tensor)\n",
    "\n",
    "        output_tensor = output_tensor.squeeze(0).cpu().numpy()\n",
    "        output_image = np.transpose(output_tensor, (1, 2, 0))\n",
    "        output_image = np.clip(output_image, 0, 1)\n",
    "\n",
    "        ground_truth_image = Image.open(gt_img_path).convert(\"RGB\")\n",
    "        ground_truth_image = np.asarray(ground_truth_image).astype(np.float32) / 255.0\n",
    "\n",
    "        output_image = cv2.resize(output_image, (ground_truth_image.shape[1], ground_truth_image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        psnr_value = calculate_psnr(output_image, ground_truth_image)\n",
    "        ssim_value = calculate_ssim(output_image, ground_truth_image)\n",
    "        metrics = calculate_precision_recall_accuracy(output_image, ground_truth_image)\n",
    "\n",
    "        psnr_values.append(psnr_value)\n",
    "        ssim_values.append(ssim_value)\n",
    "        precision_values.append(metrics[\"Precision\"])\n",
    "        recall_values.append(metrics[\"Recall\"])\n",
    "        accuracy_values.append(metrics[\"Accuracy\"])\n",
    "\n",
    "    avg_psnr = np.mean(psnr_values)\n",
    "    avg_ssim = np.mean(ssim_values)\n",
    "    avg_precision = np.mean(precision_values)\n",
    "    avg_recall = np.mean(recall_values)\n",
    "    avg_accuracy = np.mean(accuracy_values)\n",
    "\n",
    "    print(\"Average Metrics for Directory:\")\n",
    "    print(f\"PSNR: {avg_psnr:.4f}\")\n",
    "    print(f\"SSIM: {avg_ssim:.4f}\")\n",
    "    print(f\"Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": avg_psnr,\n",
    "        \"SSIM\": avg_ssim,\n",
    "        \"Precision\": avg_precision,\n",
    "        \"Recall\": avg_recall,\n",
    "        \"Accuracy\": avg_accuracy,\n",
    "    }\n",
    "\n",
    "test_image_dir = r\"anti_reflection\\dataset\\aug_blended\"\n",
    "ground_truth_dir = r\"anti_reflection\\dataset\\aug_transmission_layer\"\n",
    "model_path = r\"anti_reflection\\models\\new_model_final.pth\"\n",
    "\n",
    "metrics = test_gan_model_on_directory(model_path, test_image_dir, ground_truth_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
